# PIPELINE DEFINITION
# Name: data-preparation-pipeline
# Description: Pipeline for preparing and splitting dataset
# Inputs:
#    random_state: int [Default: 42.0]
components:
  comp-download-dataset:
    executorLabel: exec-download-dataset
    inputDefinitions:
      parameters:
        output_dir:
          defaultValue: DATASET
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-output-file-contents:
    executorLabel: exec-output-file-contents
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-split-dataset:
    executorLabel: exec-split-dataset
    inputDefinitions:
      artifacts:
        input_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        random_state:
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        test_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        validation_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-download-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'requests' 'tqdm'\
          \ 'boto3' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_dataset(output_dataset: Output[Dataset], output_dir:\
          \ str = \"DATASET\"):\n    import os\n    import requests\n    import tarfile\n\
          \    from tqdm import tqdm\n\n    # Define the final extraction path\n \
          \   extraction_path = os.path.join(output_dataset.path, output_dir)\n  \
          \  # Define a temporary path for the downloaded file inside the final extraction\
          \ directory\n    temp_download_path = os.path.join(extraction_path, \"temp_downloaded_dataset.gz\"\
          )\n\n    # Create the final output directory (and the temp directory is\
          \ inside)\n    os.makedirs(extraction_path, exist_ok=True)\n\n    # FULL\
          \ Dataset URL\n    url = \"https://manning.box.com/shared/static/34dbdkmhahuafcxh0yhiqaf05rqnzjq9.gz\"\
          \n\n    print(\"Starting download...\")\n    response = requests.get(url,\
          \ stream=True)\n    file_size = int(response.headers.get(\"Content-Length\"\
          , 0))\n    progress_bar = tqdm(total=file_size, unit=\"B\", unit_scale=True)\n\
          \n    with open(temp_download_path, 'wb') as file:\n        for chunk in\
          \ response.iter_content(chunk_size=1024):\n            progress_bar.update(len(chunk))\n\
          \            file.write(chunk)\n    progress_bar.close()\n    print(\"Download\
          \ complete.\")\n\n    # Extract to the output directory\n    print(f\"Extracting\
          \ to {extraction_path}...\")\n    with tarfile.open(temp_download_path,\
          \ 'r:gz') as tar:\n        tar.extractall(extraction_path)\n    print(\"\
          Extraction complete.\")\n\n    # Delete the temporary downloaded file\n\
          \    os.remove(temp_download_path)\n    print(\"Temporary file deleted.\"\
          )\n\n"
        image: python:3.11
    exec-output-file-contents:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - output_file_contents
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef output_file_contents(dataset: Input[Dataset]):\n    import os\n\
          \n    def list_files(start_path):\n        for root, dirs, files in os.walk(start_path):\n\
          \            level = root.replace(start_path, '').count(os.sep)\n      \
          \      indent = ' ' * 4 * (level)\n            print(f'{indent}{os.path.basename(root)}/')\n\
          \            sub_indent = ' ' * 4 * (level + 1)\n            for f in files:\n\
          \                print(f'{sub_indent}{f}')\n\n    print(f\"Contents of {dataset.path}:\"\
          )\n    list_files(dataset.path)\n\n"
        image: python:3.11
    exec-split-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - split_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.11.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'scikit-learn'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef split_dataset(\n        random_state: int,\n        input_dataset:\
          \ Input[Dataset],\n        train_dataset: Output[Dataset],  # Changed output\
          \ names\n        validation_dataset: Output[Dataset],\n        test_dataset:\
          \ Output[Dataset]\n):\n    import os\n    import glob\n    import shutil\n\
          \    from sklearn.model_selection import train_test_split\n\n    BASE_PATH\
          \ = \"MINIDATA\"\n    # BASE_PATH = \"DATA\"\n\n    # Adjust paths to use\
          \ input_dataset.path\n    images = list(glob.glob(os.path.join(input_dataset.path,\
          \ \"DATASET\", BASE_PATH, \"images\", \"**\")))\n    labels = list(glob.glob(os.path.join(input_dataset.path,\
          \ \"DATASET\", BASE_PATH, \"labels\", \"**\")))\n\n    train_ratio = 0.75\n\
          \    validation_ratio = 0.15\n    test_ratio = 0.10\n\n    # train is now\
          \ 75% of the entire data set\n    x_train, x_test, y_train, y_test = train_test_split(\n\
          \        images,\n        labels,\n        test_size=1 - train_ratio,\n\
          \        random_state=random_state\n    )\n\n    # test is now 10% of the\
          \ initial data set\n    # validation is now 15% of the initial data set\n\
          \    x_val, x_test, y_val, y_test = train_test_split(\n        x_test,\n\
          \        y_test,\n        test_size=test_ratio / (test_ratio + validation_ratio),\n\
          \        random_state=random_state\n    )\n\n    # Create output directories\
          \ for each split\n    for dataset_output, x_files, y_files in [\n      \
          \  (train_dataset, x_train, y_train),\n        (validation_dataset, x_val,\
          \ y_val),\n        (test_dataset, x_test, y_test)\n    ]:\n        os.makedirs(os.path.join(dataset_output.path,\
          \ \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(dataset_output.path,\
          \ \"labels\"), exist_ok=True)\n\n        # Move files\n        for src in\
          \ x_files:\n            dest = os.path.join(dataset_output.path, \"images\"\
          , os.path.basename(src))\n            shutil.copy2(src, dest)\n\n      \
          \  for src in y_files:\n            dest = os.path.join(dataset_output.path,\
          \ \"labels\", os.path.basename(src))\n            shutil.copy2(src, dest)\n\
          \n"
        image: python:3.11
pipelineInfo:
  description: Pipeline for preparing and splitting dataset
  name: data-preparation-pipeline
root:
  dag:
    tasks:
      download-dataset:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-dataset
        taskInfo:
          name: download-dataset
      output-file-contents:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-output-file-contents
        dependentTasks:
        - split-dataset
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: train_dataset
                producerTask: split-dataset
        taskInfo:
          name: output-file-contents
      split-dataset:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-split-dataset
        dependentTasks:
        - download-dataset
        inputs:
          artifacts:
            input_dataset:
              taskOutputArtifact:
                outputArtifactKey: output_dataset
                producerTask: download-dataset
          parameters:
            random_state:
              componentInputParameter: random_state
        taskInfo:
          name: split-dataset
  inputDefinitions:
    parameters:
      random_state:
        defaultValue: 42.0
        isOptional: true
        parameterType: NUMBER_INTEGER
schemaVersion: 2.1.0
sdkVersion: kfp-2.11.0
