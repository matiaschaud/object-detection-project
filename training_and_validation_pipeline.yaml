apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: yolo-object-detection-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org/pipeline_compilation_time: '2023-10-16T00:10:48.253012',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "YOLO Object Detection
      Pipeline", "inputs": [{"default": "1", "name": "epochs", "optional": true, "type":
      "Integer"}, {"default": "8", "name": "batch", "optional": true, "type": "Integer"},
      {"default": "42", "name": "random_state", "optional": true, "type": "Integer"},
      {"default": "yolov8n_custom", "name": "yolo_model_name", "optional": true, "type":
      "String"}], "name": "YOLO Object Detection Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22}
spec:
  entrypoint: yolo-object-detection-pipeline
  templates:
  - name: create-pvc
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-pipeline-pvc'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 20Gi
    outputs:
      parameters:
      - name: create-pvc-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-pvc-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-pvc-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: download-dataset
    container:
      args: [--output-dir, DATASET]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'requests' 'boto3' 'tqdm' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
        install --quiet --no-warn-script-location 'requests' 'boto3' 'tqdm' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def download_dataset(output_dir = "DATASET"):
            import os
            import requests
            import tarfile
            from tqdm import tqdm

            # FULL Dataset
            url = "https://manning.box.com/shared/static/34dbdkmhahuafcxh0yhiqaf05rqnzjq9.gz"

            downloaded_file = "DATASET.gz"

            response = requests.get(url, stream=True)
            file_size = int(response.headers.get("Content-Length", 0))
            progress_bar = tqdm(total=file_size, unit="B", unit_scale=True)

            with open(downloaded_file, 'wb') as file:
                for chunk in response.iter_content(chunk_size=1024):
                    # Update the progress bar with the size of the downloaded chunk
                    progress_bar.update(len(chunk))
                    file.write(chunk)

            # Open the tar archive
            with tarfile.open(downloaded_file, 'r:gz') as tar:
                # Extract all files from the archive
                tar.extractall(os.path.join("/", "mnt", "pipeline", output_dir))

        import argparse
        _parser = argparse.ArgumentParser(prog='Download dataset', description='')
        _parser.add_argument("--output-dir", dest="output_dir", type=str, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = download_dataset(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: /mnt/pipeline, name: local-storage}
    inputs:
      parameters:
      - {name: create-pvc-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "output_dir"}, "then": ["--output-dir",
          {"inputValue": "output_dir"}]}}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''requests'' ''boto3''
          ''tqdm'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''requests'' ''boto3'' ''tqdm'' --user) && \"$0\"
          \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def download_dataset(output_dir = \"DATASET\"):\n    import
          os\n    import requests\n    import tarfile\n    from tqdm import tqdm\n\n    #
          FULL Dataset\n    url = \"https://manning.box.com/shared/static/34dbdkmhahuafcxh0yhiqaf05rqnzjq9.gz\"\n\n    downloaded_file
          = \"DATASET.gz\"\n\n    response = requests.get(url, stream=True)\n    file_size
          = int(response.headers.get(\"Content-Length\", 0))\n    progress_bar = tqdm(total=file_size,
          unit=\"B\", unit_scale=True)\n\n    with open(downloaded_file, ''wb'') as
          file:\n        for chunk in response.iter_content(chunk_size=1024):\n            #
          Update the progress bar with the size of the downloaded chunk\n            progress_bar.update(len(chunk))\n            file.write(chunk)\n\n    #
          Open the tar archive\n    with tarfile.open(downloaded_file, ''r:gz'') as
          tar:\n        # Extract all files from the archive\n        tar.extractall(os.path.join(\"/\",
          \"mnt\", \"pipeline\", output_dir))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Download
          dataset'', description='''')\n_parser.add_argument(\"--output-dir\", dest=\"output_dir\",
          type=str, required=False, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = download_dataset(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"default": "DATASET", "name": "output_dir", "optional": true, "type":
          "String"}], "name": "Download dataset"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"output_dir": "DATASET"}'}
    volumes:
    - name: local-storage
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-pvc-name}}'}
  - name: split-dataset
    container:
      args: [--random-state, '{{inputs.parameters.random_state}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio' 'scikit-learn' 'tqdm' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m
        pip install --quiet --no-warn-script-location 'minio' 'scikit-learn' 'tqdm'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def split_dataset(random_state):

            import os
            import glob
            from tqdm import tqdm
            import shutil

            images = list(glob.glob(os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "images", "**")))
            labels = list(glob.glob(os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "labels", "**")))

            from sklearn.model_selection import train_test_split

            train_ratio = 0.75
            validation_ratio = 0.15
            test_ratio = 0.10

            # train is now 75% of the entire data set
            x_train, x_test, y_train, y_test = train_test_split(images, labels,
                                                                test_size=1 - train_ratio,
                                                                random_state=random_state)

            # test is now 10% of the initial data set
            # validation is now 15% of the initial data set
            x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,
                                                            test_size=test_ratio / (test_ratio + validation_ratio),
                                                            random_state=random_state)

            for splits in ["train", "test", "val"]:
                for x in ["images", "labels"]:
                    os.makedirs(os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", splits, x))

            for source_object in x_train:
                src = source_object.strip()
                dest = os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "train", "images", os.path.basename(source_object))
                shutil.move(src, dest)

            for source_object in x_test:
                src = source_object.strip()
                dest = os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "test", "images",
                                    os.path.basename(source_object))
                shutil.move(src, dest)

            for source_object in x_val:
                src = source_object.strip()
                dest = os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "val", "images",
                                    os.path.basename(source_object))
                shutil.move(src, dest)

            for source_object in y_train:
                src = source_object.strip()
                dest = os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "train", "labels",
                                    os.path.basename(source_object))
                shutil.move(src, dest)

            for source_object in y_test:
                src = source_object.strip()
                dest = os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "test", "labels",
                                    os.path.basename(source_object))
                shutil.move(src, dest)

            for source_object in y_val:
                src = source_object.strip()
                dest = os.path.join("/", "mnt", "pipeline", "DATASET", "DATA", "val", "labels",
                                    os.path.basename(source_object))
                shutil.move(src, dest)

        import argparse
        _parser = argparse.ArgumentParser(prog='Split dataset', description='')
        _parser.add_argument("--random-state", dest="random_state", type=int, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = split_dataset(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: /mnt/pipeline, name: local-storage}
    inputs:
      parameters:
      - {name: create-pvc-name}
      - {name: random_state}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--random-state", {"inputValue": "random_state"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''minio'' ''scikit-learn'' ''tqdm'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio'' ''scikit-learn''
          ''tqdm'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def split_dataset(random_state):\n\n    import os\n    import glob\n    from
          tqdm import tqdm\n    import shutil\n\n    images = list(glob.glob(os.path.join(\"/\",
          \"mnt\", \"pipeline\", \"DATASET\", \"DATA\", \"images\", \"**\")))\n    labels
          = list(glob.glob(os.path.join(\"/\", \"mnt\", \"pipeline\", \"DATASET\",
          \"DATA\", \"labels\", \"**\")))\n\n    from sklearn.model_selection import
          train_test_split\n\n    train_ratio = 0.75\n    validation_ratio = 0.15\n    test_ratio
          = 0.10\n\n    # train is now 75% of the entire data set\n    x_train, x_test,
          y_train, y_test = train_test_split(images, labels,\n                                                        test_size=1
          - train_ratio,\n                                                        random_state=random_state)\n\n    #
          test is now 10% of the initial data set\n    # validation is now 15% of
          the initial data set\n    x_val, x_test, y_val, y_test = train_test_split(x_test,
          y_test,\n                                                    test_size=test_ratio
          / (test_ratio + validation_ratio),\n                                                    random_state=random_state)\n\n    for
          splits in [\"train\", \"test\", \"val\"]:\n        for x in [\"images\",
          \"labels\"]:\n            os.makedirs(os.path.join(\"/\", \"mnt\", \"pipeline\",
          \"DATASET\", \"DATA\", splits, x))\n\n    for source_object in x_train:\n        src
          = source_object.strip()\n        dest = os.path.join(\"/\", \"mnt\", \"pipeline\",
          \"DATASET\", \"DATA\", \"train\", \"images\", os.path.basename(source_object))\n        shutil.move(src,
          dest)\n\n    for source_object in x_test:\n        src = source_object.strip()\n        dest
          = os.path.join(\"/\", \"mnt\", \"pipeline\", \"DATASET\", \"DATA\", \"test\",
          \"images\",\n                            os.path.basename(source_object))\n        shutil.move(src,
          dest)\n\n    for source_object in x_val:\n        src = source_object.strip()\n        dest
          = os.path.join(\"/\", \"mnt\", \"pipeline\", \"DATASET\", \"DATA\", \"val\",
          \"images\",\n                            os.path.basename(source_object))\n        shutil.move(src,
          dest)\n\n    for source_object in y_train:\n        src = source_object.strip()\n        dest
          = os.path.join(\"/\", \"mnt\", \"pipeline\", \"DATASET\", \"DATA\", \"train\",
          \"labels\",\n                            os.path.basename(source_object))\n        shutil.move(src,
          dest)\n\n    for source_object in y_test:\n        src = source_object.strip()\n        dest
          = os.path.join(\"/\", \"mnt\", \"pipeline\", \"DATASET\", \"DATA\", \"test\",
          \"labels\",\n                            os.path.basename(source_object))\n        shutil.move(src,
          dest)\n\n    for source_object in y_val:\n        src = source_object.strip()\n        dest
          = os.path.join(\"/\", \"mnt\", \"pipeline\", \"DATASET\", \"DATA\", \"val\",
          \"labels\",\n                            os.path.basename(source_object))\n        shutil.move(src,
          dest)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Split
          dataset'', description='''')\n_parser.add_argument(\"--random-state\", dest=\"random_state\",
          type=int, required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = split_dataset(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "random_state", "type": "Integer"}], "name": "Split dataset"}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"random_state":
          "{{inputs.parameters.random_state}}"}'}
    volumes:
    - name: local-storage
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-pvc-name}}'}
  - name: train-model
    container:
      args: [--epochs, '{{inputs.parameters.epochs}}', --batch, '{{inputs.parameters.batch}}',
        --yolo-model-name, '{{inputs.parameters.yolo_model_name}}', --data-yaml, /tmp/outputs/data_yaml/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio' 'tqdm' 'pyyaml' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
        install --quiet --no-warn-script-location 'minio' 'tqdm' 'pyyaml' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model(epochs,
                        batch,
                        yolo_model_name,
                        data_yaml_path,
                        ):

            import os
            import yaml

            data = {
                'path': '/mnt/pipeline/DATASET/DATA',
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'names': {
                    0: 'id_card'
                }
            }

            data_yaml_full_path = os.path.join(data_yaml_path, "data.yaml")
            from pathlib import Path
            Path(data_yaml_path).mkdir(parents=True, exist_ok=True)

            try:
                with open(data_yaml_full_path, 'w') as file:
                    yaml.dump(data, file)
                print("YAML file has been written successfully.")
            except Exception as e:
                print(f"Error writing YAML file: {e}")

            from ultralytics import YOLO

            model = YOLO('yolov8n.pt')

            results = model.train(
                data=data_yaml_full_path,
                imgsz=640,
                epochs=epochs,
                batch=batch,
                project="/mnt/pipeline",
                name=yolo_model_name,
           )

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model', description='')
        _parser.add_argument("--epochs", dest="epochs", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch", dest="batch", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--yolo-model-name", dest="yolo_model_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-yaml", dest="data_yaml_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model(**_parsed_args)
      image: ultralytics/ultralytics:8.0.194-cpu
      volumeMounts:
      - {mountPath: /mnt/pipeline, name: local-storage}
    inputs:
      parameters:
      - {name: batch}
      - {name: create-pvc-name}
      - {name: epochs}
      - {name: yolo_model_name}
    outputs:
      artifacts:
      - {name: train-model-data_yaml, path: /tmp/outputs/data_yaml/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--epochs", {"inputValue": "epochs"}, "--batch", {"inputValue":
          "batch"}, "--yolo-model-name", {"inputValue": "yolo_model_name"}, "--data-yaml",
          {"outputPath": "data_yaml"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio'' ''tqdm''
          ''pyyaml'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''minio'' ''tqdm'' ''pyyaml'' --user) && \"$0\"
          \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train_model(epochs,\n                batch,\n                yolo_model_name,\n                data_yaml_path,\n                ):\n\n    import
          os\n    import yaml\n\n    data = {\n        ''path'': ''/mnt/pipeline/DATASET/DATA'',\n        ''train'':
          ''train/images'',\n        ''val'': ''val/images'',\n        ''test'': ''test/images'',\n        ''names'':
          {\n            0: ''id_card''\n        }\n    }\n\n    data_yaml_full_path
          = os.path.join(data_yaml_path, \"data.yaml\")\n    from pathlib import Path\n    Path(data_yaml_path).mkdir(parents=True,
          exist_ok=True)\n\n    try:\n        with open(data_yaml_full_path, ''w'')
          as file:\n            yaml.dump(data, file)\n        print(\"YAML file has
          been written successfully.\")\n    except Exception as e:\n        print(f\"Error
          writing YAML file: {e}\")\n\n    from ultralytics import YOLO\n\n    model
          = YOLO(''yolov8n.pt'')\n\n    results = model.train(\n        data=data_yaml_full_path,\n        imgsz=640,\n        epochs=epochs,\n        batch=batch,\n        project=\"/mnt/pipeline\",\n        name=yolo_model_name,\n   )\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch\",
          dest=\"batch\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--yolo-model-name\",
          dest=\"yolo_model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-yaml\",
          dest=\"data_yaml_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "ultralytics/ultralytics:8.0.194-cpu"}},
          "inputs": [{"name": "epochs", "type": "Integer"}, {"name": "batch", "type":
          "Integer"}, {"name": "yolo_model_name", "type": "String"}], "name": "Train
          model", "outputs": [{"name": "data_yaml", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"batch": "{{inputs.parameters.batch}}",
          "epochs": "{{inputs.parameters.epochs}}", "yolo_model_name": "{{inputs.parameters.yolo_model_name}}"}'}
    volumes:
    - name: local-storage
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-pvc-name}}'}
  - name: validate-model
    container:
      args: [--data-yaml, /tmp/inputs/data_yaml/data, --yolo-model-name, '{{inputs.parameters.yolo_model_name}}',
        --mlpipeline-metrics, /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio' 'tqdm' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'minio' 'tqdm' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def validate_model(data_yaml_path,
                           yolo_model_name,
                           mlpipeline_metrics_path):

            from ultralytics import YOLO
            import json
            import os

            weights_path = os.path.join("/", "mnt", "pipeline", yolo_model_name, "weights", "best.pt")
            print(f"Loading weights at: {weights_path}")

            model = YOLO(weights_path)

            metrics = model.val(
                data=os.path.join(data_yaml_path, "data.yaml")
            )
            metrics_dict = {
                "metrics": [
                    {
                        "name": "map50-95",
                        "numberValue": metrics.box.map,
                        "format": "RAW",
                    },
                    {
                        "name": "map50",
                        "numberValue": metrics.box.map50,
                        "format": "RAW",
                    },
                    {
                        "name": "map75",
                        "numberValue": metrics.box.map75,
                        "format": "RAW",
                    }]
            }

            with open(mlpipeline_metrics_path, "w") as f:
                json.dump(metrics_dict, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate model', description='')
        _parser.add_argument("--data-yaml", dest="data_yaml_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--yolo-model-name", dest="yolo_model_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-metrics", dest="mlpipeline_metrics_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = validate_model(**_parsed_args)
      image: ultralytics/ultralytics:8.0.194-cpu
      volumeMounts:
      - {mountPath: /mnt/pipeline, name: local-storage}
    inputs:
      parameters:
      - {name: create-pvc-name}
      - {name: yolo_model_name}
      artifacts:
      - {name: train-model-data_yaml, path: /tmp/inputs/data_yaml/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-yaml", {"inputPath": "data_yaml"}, "--yolo-model-name",
          {"inputValue": "yolo_model_name"}, "--mlpipeline-metrics", {"outputPath":
          "mlpipeline_metrics"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio'' ''tqdm''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''minio'' ''tqdm'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef validate_model(data_yaml_path,\n                   yolo_model_name,\n                   mlpipeline_metrics_path):\n\n    from
          ultralytics import YOLO\n    import json\n    import os\n\n    weights_path
          = os.path.join(\"/\", \"mnt\", \"pipeline\", yolo_model_name, \"weights\",
          \"best.pt\")\n    print(f\"Loading weights at: {weights_path}\")\n\n    model
          = YOLO(weights_path)\n\n    metrics = model.val(\n        data=os.path.join(data_yaml_path,
          \"data.yaml\")\n    )\n    metrics_dict = {\n        \"metrics\": [\n            {\n                \"name\":
          \"map50-95\",\n                \"numberValue\": metrics.box.map,\n                \"format\":
          \"RAW\",\n            },\n            {\n                \"name\": \"map50\",\n                \"numberValue\":
          metrics.box.map50,\n                \"format\": \"RAW\",\n            },\n            {\n                \"name\":
          \"map75\",\n                \"numberValue\": metrics.box.map75,\n                \"format\":
          \"RAW\",\n            }]\n    }\n\n    with open(mlpipeline_metrics_path,
          \"w\") as f:\n        json.dump(metrics_dict, f)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Validate model'', description='''')\n_parser.add_argument(\"--data-yaml\",
          dest=\"data_yaml_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--yolo-model-name\",
          dest=\"yolo_model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\",
          dest=\"mlpipeline_metrics_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = validate_model(**_parsed_args)\n"], "image": "ultralytics/ultralytics:8.0.194-cpu"}},
          "inputs": [{"name": "data_yaml", "type": "String"}, {"name": "yolo_model_name",
          "type": "String"}], "name": "Validate model", "outputs": [{"name": "mlpipeline_metrics",
          "type": "Metrics"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"yolo_model_name":
          "{{inputs.parameters.yolo_model_name}}"}'}
    volumes:
    - name: local-storage
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-pvc-name}}'}
  - name: yolo-object-detection-pipeline
    inputs:
      parameters:
      - {name: batch}
      - {name: epochs}
      - {name: random_state}
      - {name: yolo_model_name}
    dag:
      tasks:
      - {name: create-pvc, template: create-pvc}
      - name: download-dataset
        template: download-dataset
        dependencies: [create-pvc]
        arguments:
          parameters:
          - {name: create-pvc-name, value: '{{tasks.create-pvc.outputs.parameters.create-pvc-name}}'}
      - name: split-dataset
        template: split-dataset
        dependencies: [create-pvc, download-dataset]
        arguments:
          parameters:
          - {name: create-pvc-name, value: '{{tasks.create-pvc.outputs.parameters.create-pvc-name}}'}
          - {name: random_state, value: '{{inputs.parameters.random_state}}'}
      - name: train-model
        template: train-model
        dependencies: [create-pvc, split-dataset]
        arguments:
          parameters:
          - {name: batch, value: '{{inputs.parameters.batch}}'}
          - {name: create-pvc-name, value: '{{tasks.create-pvc.outputs.parameters.create-pvc-name}}'}
          - {name: epochs, value: '{{inputs.parameters.epochs}}'}
          - {name: yolo_model_name, value: '{{inputs.parameters.yolo_model_name}}'}
      - name: validate-model
        template: validate-model
        dependencies: [create-pvc, train-model]
        arguments:
          parameters:
          - {name: create-pvc-name, value: '{{tasks.create-pvc.outputs.parameters.create-pvc-name}}'}
          - {name: yolo_model_name, value: '{{inputs.parameters.yolo_model_name}}'}
          artifacts:
          - {name: train-model-data_yaml, from: '{{tasks.train-model.outputs.artifacts.train-model-data_yaml}}'}
  arguments:
    parameters:
    - {name: epochs, value: '1'}
    - {name: batch, value: '8'}
    - {name: random_state, value: '42'}
    - {name: yolo_model_name, value: yolov8n_custom}
  serviceAccountName: pipeline-runner
