apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: yolo-object-detection-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org/pipeline_compilation_time: '2023-10-15T11:53:44.277500',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "YOLO Object Detection
      Pipeline", "inputs": [{"default": "1", "name": "epochs", "optional": true, "type":
      "Integer"}, {"default": "8", "name": "batch", "optional": true, "type": "Integer"},
      {"default": "dataset", "name": "source_bucket", "optional": true}, {"default":
      "42", "name": "random_state", "optional": true, "type": "Integer"}, {"default":
      "yolov8n_custom", "name": "yolo_model_name", "optional": true, "type": "String"}],
      "name": "YOLO Object Detection Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22}
spec:
  entrypoint: yolo-object-detection-pipeline
  templates:
  - name: create-pvc
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-pipeline-pvc'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
    outputs:
      parameters:
      - name: create-pvc-manifest
        valueFrom: {jsonPath: '{}'}
      - name: create-pvc-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: create-pvc-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: split-dataset
    container:
      args: [--bucket-name, '{{inputs.parameters.source_bucket}}', --random-state,
        '{{inputs.parameters.random_state}}', --x-train, /tmp/outputs/x_train/data,
        --y-train, /tmp/outputs/y_train/data, --x-test, /tmp/outputs/x_test/data,
        --y-test, /tmp/outputs/y_test/data, --x-val, /tmp/outputs/x_val/data, --y-val,
        /tmp/outputs/y_val/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio' 'scikit-learn' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'minio' 'scikit-learn' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def split_dataset(bucket_name,
                          random_state,
                          x_train_file,
                          y_train_file,
                          x_test_file,
                          y_test_file,
                          x_val_file,
                          y_val_file):

            from minio import Minio
            from minio.error import S3Error

            def list_objects_with_prefix(minio_client, bucket_name, prefix):
                try:
                    objects = minio_client.list_objects(bucket_name, prefix=prefix, recursive=True)
                    return [obj.object_name for obj in objects]
                except S3Error as err:
                    print(f"Error listing objects with prefix '{prefix}' in bucket '{bucket_name}': {err}")

            minio_client = Minio(
                'minio-service.kubeflow:9000',
                access_key='minio',
                secret_key='minio123',
                secure=False
            )

            images = list_objects_with_prefix(minio_client, bucket_name, prefix=f"{bucket_name}/images")
            labels = list_objects_with_prefix(minio_client, bucket_name, prefix=f"{bucket_name}/labels")

            from sklearn.model_selection import train_test_split

            train_ratio = 0.75
            validation_ratio = 0.15
            test_ratio = 0.10

            # train is now 75% of the entire data set
            x_train, x_test, y_train, y_test = train_test_split(images, labels,
                                                                test_size=1 - train_ratio,
                                                                random_state=random_state)

            # test is now 10% of the initial data set
            # validation is now 15% of the initial data set
            x_val, x_test, y_val, y_test = train_test_split(x_test, y_test,
                                                            test_size=test_ratio / (test_ratio + validation_ratio),
                                                            random_state=random_state)

            with open(x_train_file, "w") as f:
                f.writelines(line + '\n' for line in x_train)

            with open(y_train_file, "w") as f:
                f.writelines(line + '\n' for line in y_train)

            with open(x_test_file, "w") as f:
                f.writelines(line + '\n' for line in x_test)

            with open(y_test_file, "w") as f:
                f.writelines(line + '\n' for line in y_test)

            with open(x_val_file, "w") as f:
                f.writelines(line + '\n' for line in x_val)

            with open(y_val_file, "w") as f:
                f.writelines(line + '\n' for line in y_val)

            print(len(x_train))
            print(len(x_val))
            print(len(x_test))

        import argparse
        _parser = argparse.ArgumentParser(prog='Split dataset', description='')
        _parser.add_argument("--bucket-name", dest="bucket_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--random-state", dest="random_state", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--x-train", dest="x_train_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-train", dest="y_train_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--x-test", dest="x_test_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-test", dest="y_test_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--x-val", dest="x_val_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-val", dest="y_val_file", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = split_dataset(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: random_state}
      - {name: source_bucket}
    outputs:
      artifacts:
      - {name: split-dataset-x_test, path: /tmp/outputs/x_test/data}
      - {name: split-dataset-x_train, path: /tmp/outputs/x_train/data}
      - {name: split-dataset-x_val, path: /tmp/outputs/x_val/data}
      - {name: split-dataset-y_test, path: /tmp/outputs/y_test/data}
      - {name: split-dataset-y_train, path: /tmp/outputs/y_train/data}
      - {name: split-dataset-y_val, path: /tmp/outputs/y_val/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bucket-name", {"inputValue": "bucket_name"}, "--random-state",
          {"inputValue": "random_state"}, "--x-train", {"outputPath": "x_train"},
          "--y-train", {"outputPath": "y_train"}, "--x-test", {"outputPath": "x_test"},
          "--y-test", {"outputPath": "y_test"}, "--x-val", {"outputPath": "x_val"},
          "--y-val", {"outputPath": "y_val"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio'' ''scikit-learn''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''minio'' ''scikit-learn'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef split_dataset(bucket_name,\n                  random_state,\n                  x_train_file,\n                  y_train_file,\n                  x_test_file,\n                  y_test_file,\n                  x_val_file,\n                  y_val_file):\n\n    from
          minio import Minio\n    from minio.error import S3Error\n\n    def list_objects_with_prefix(minio_client,
          bucket_name, prefix):\n        try:\n            objects = minio_client.list_objects(bucket_name,
          prefix=prefix, recursive=True)\n            return [obj.object_name for
          obj in objects]\n        except S3Error as err:\n            print(f\"Error
          listing objects with prefix ''{prefix}'' in bucket ''{bucket_name}'': {err}\")\n\n    minio_client
          = Minio(\n        ''minio-service.kubeflow:9000'',\n        access_key=''minio'',\n        secret_key=''minio123'',\n        secure=False\n    )\n\n    images
          = list_objects_with_prefix(minio_client, bucket_name, prefix=f\"{bucket_name}/images\")\n    labels
          = list_objects_with_prefix(minio_client, bucket_name, prefix=f\"{bucket_name}/labels\")\n\n    from
          sklearn.model_selection import train_test_split\n\n    train_ratio = 0.75\n    validation_ratio
          = 0.15\n    test_ratio = 0.10\n\n    # train is now 75% of the entire data
          set\n    x_train, x_test, y_train, y_test = train_test_split(images, labels,\n                                                        test_size=1
          - train_ratio,\n                                                        random_state=random_state)\n\n    #
          test is now 10% of the initial data set\n    # validation is now 15% of
          the initial data set\n    x_val, x_test, y_val, y_test = train_test_split(x_test,
          y_test,\n                                                    test_size=test_ratio
          / (test_ratio + validation_ratio),\n                                                    random_state=random_state)\n\n    with
          open(x_train_file, \"w\") as f:\n        f.writelines(line + ''\\n'' for
          line in x_train)\n\n    with open(y_train_file, \"w\") as f:\n        f.writelines(line
          + ''\\n'' for line in y_train)\n\n    with open(x_test_file, \"w\") as f:\n        f.writelines(line
          + ''\\n'' for line in x_test)\n\n    with open(y_test_file, \"w\") as f:\n        f.writelines(line
          + ''\\n'' for line in y_test)\n\n    with open(x_val_file, \"w\") as f:\n        f.writelines(line
          + ''\\n'' for line in x_val)\n\n    with open(y_val_file, \"w\") as f:\n        f.writelines(line
          + ''\\n'' for line in y_val)\n\n    print(len(x_train))\n    print(len(x_val))\n    print(len(x_test))\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Split dataset'', description='''')\n_parser.add_argument(\"--bucket-name\",
          dest=\"bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--random-state\",
          dest=\"random_state\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--x-train\",
          dest=\"x_train_file\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-train\", dest=\"y_train_file\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--x-test\",
          dest=\"x_test_file\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-test\", dest=\"y_test_file\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--x-val\",
          dest=\"x_val_file\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-val\", dest=\"y_val_file\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = split_dataset(**_parsed_args)\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "bucket_name", "type": "String"},
          {"name": "random_state", "type": "Integer"}], "name": "Split dataset", "outputs":
          [{"name": "x_train", "type": "String"}, {"name": "y_train", "type": "String"},
          {"name": "x_test", "type": "String"}, {"name": "y_test", "type": "String"},
          {"name": "x_val", "type": "String"}, {"name": "y_val", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bucket_name":
          "{{inputs.parameters.source_bucket}}", "random_state": "{{inputs.parameters.random_state}}"}'}
  - name: train-model
    container:
      args: [--epochs, '{{inputs.parameters.epochs}}', --batch, '{{inputs.parameters.batch}}',
        --source-bucket, dataset, --yolo-model-name, '{{inputs.parameters.yolo_model_name}}',
        --x-train, /tmp/inputs/x_train/data, --y-train, /tmp/inputs/y_train/data,
        --x-test, /tmp/inputs/x_test/data, --y-test, /tmp/inputs/y_test/data, --x-val,
        /tmp/inputs/x_val/data, --y-val, /tmp/inputs/y_val/data, --data-yaml, /tmp/outputs/data_yaml/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio' 'tqdm' 'pyyaml' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
        install --quiet --no-warn-script-location 'minio' 'tqdm' 'pyyaml' --user)
        && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_model(epochs,
                        batch,
                        source_bucket,
                        yolo_model_name,
                        x_train_file,
                        y_train_file,
                        x_test_file,
                        y_test_file,
                        x_val_file,
                        y_val_file,
                        data_yaml_path,
                        ):
            from minio import Minio
            from minio.error import S3Error
            from tqdm import tqdm
            import os
            import yaml

            def download_from_minio(source_bucket, source_object, minio_client, download_path):
                try:
                    # Download the file from MinIO
                    minio_client.fget_object(source_bucket, source_object, download_path)
                except S3Error as err:
                    print(f"Error downloading {source_object}: {err}")

            minio_client = Minio(
                'minio-service.kubeflow:9000',
                access_key='minio',
                secret_key='minio123',
                secure=False
            )

            # Create local directories.
            for splits in ["train", "test", "val"]:
                for x in ["images", "labels"]:
                    os.makedirs(f"/dataset/{splits}/{x}", exist_ok=True)

            Xs = [x_train_file, x_test_file, x_val_file]
            Ys = [y_train_file, y_test_file, y_val_file]

            for i, splits in enumerate(["train", "test", "val"]):
                # Download image
                with open(Xs[i], "r") as f:
                    for source_object in tqdm(f.readlines()):
                        source_object = source_object.strip()
                        download_path = f"/dataset/{splits}/images/{os.path.basename(source_object)}"
                        download_from_minio(source_bucket, source_object, minio_client, download_path)

                # Download label
                with open(Ys[i], "r") as f:
                    for source_object in f.readlines():
                        source_object = source_object.strip()
                        download_path = f"/dataset/{splits}/labels/{os.path.basename(source_object)}"
                        download_from_minio(source_bucket, source_object, minio_client, download_path)

            data = {
                'path': '/dataset/',
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'names': {
                    0: 'id_card'
                }
            }

            data_yaml_full_path = os.path.join(data_yaml_path, "data.yaml")
            from pathlib import Path
            Path(data_yaml_path).mkdir(parents=True, exist_ok=True)

            try:
                with open(data_yaml_full_path, 'w') as file:
                    yaml.dump(data, file)
                print("YAML file has been written successfully.")
            except Exception as e:
                print(f"Error writing YAML file: {e}")

            from ultralytics import YOLO
            # from ultralytics import settings
            #
            # # Update a setting
            # settings.update({'mlflow': True})
            #
            # # Reset settings to default values
            # settings.reset()

            model = YOLO('yolov8n.pt')

            results = model.train(
                data=data_yaml_full_path,
                imgsz=640,
                epochs=epochs,
                batch=batch,
                project="/mnt/pipeline",
                name=yolo_model_name,
           )

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model', description='')
        _parser.add_argument("--epochs", dest="epochs", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--batch", dest="batch", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--source-bucket", dest="source_bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--yolo-model-name", dest="yolo_model_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--x-train", dest="x_train_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-train", dest="y_train_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--x-test", dest="x_test_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-test", dest="y_test_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--x-val", dest="x_val_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-val", dest="y_val_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--data-yaml", dest="data_yaml_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_model(**_parsed_args)
      image: ultralytics/ultralytics:8.0.194-cpu
      volumeMounts:
      - {mountPath: /mnt/pipeline, name: local-storage}
    inputs:
      parameters:
      - {name: batch}
      - {name: create-pvc-name}
      - {name: epochs}
      - {name: yolo_model_name}
      artifacts:
      - {name: split-dataset-x_test, path: /tmp/inputs/x_test/data}
      - {name: split-dataset-x_train, path: /tmp/inputs/x_train/data}
      - {name: split-dataset-x_val, path: /tmp/inputs/x_val/data}
      - {name: split-dataset-y_test, path: /tmp/inputs/y_test/data}
      - {name: split-dataset-y_train, path: /tmp/inputs/y_train/data}
      - {name: split-dataset-y_val, path: /tmp/inputs/y_val/data}
    outputs:
      artifacts:
      - {name: train-model-data_yaml, path: /tmp/outputs/data_yaml/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--epochs", {"inputValue": "epochs"}, "--batch", {"inputValue":
          "batch"}, "--source-bucket", {"inputValue": "source_bucket"}, "--yolo-model-name",
          {"inputValue": "yolo_model_name"}, "--x-train", {"inputPath": "x_train"},
          "--y-train", {"inputPath": "y_train"}, "--x-test", {"inputPath": "x_test"},
          "--y-test", {"inputPath": "y_test"}, "--x-val", {"inputPath": "x_val"},
          "--y-val", {"inputPath": "y_val"}, "--data-yaml", {"outputPath": "data_yaml"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''minio'' ''tqdm'' ''pyyaml''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''minio'' ''tqdm'' ''pyyaml'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef train_model(epochs,\n                batch,\n                source_bucket,\n                yolo_model_name,\n                x_train_file,\n                y_train_file,\n                x_test_file,\n                y_test_file,\n                x_val_file,\n                y_val_file,\n                data_yaml_path,\n                ):\n    from
          minio import Minio\n    from minio.error import S3Error\n    from tqdm import
          tqdm\n    import os\n    import yaml\n\n    def download_from_minio(source_bucket,
          source_object, minio_client, download_path):\n        try:\n            #
          Download the file from MinIO\n            minio_client.fget_object(source_bucket,
          source_object, download_path)\n        except S3Error as err:\n            print(f\"Error
          downloading {source_object}: {err}\")\n\n    minio_client = Minio(\n        ''minio-service.kubeflow:9000'',\n        access_key=''minio'',\n        secret_key=''minio123'',\n        secure=False\n    )\n\n    #
          Create local directories.\n    for splits in [\"train\", \"test\", \"val\"]:\n        for
          x in [\"images\", \"labels\"]:\n            os.makedirs(f\"/dataset/{splits}/{x}\",
          exist_ok=True)\n\n    Xs = [x_train_file, x_test_file, x_val_file]\n    Ys
          = [y_train_file, y_test_file, y_val_file]\n\n    for i, splits in enumerate([\"train\",
          \"test\", \"val\"]):\n        # Download image\n        with open(Xs[i],
          \"r\") as f:\n            for source_object in tqdm(f.readlines()):\n                source_object
          = source_object.strip()\n                download_path = f\"/dataset/{splits}/images/{os.path.basename(source_object)}\"\n                download_from_minio(source_bucket,
          source_object, minio_client, download_path)\n\n        # Download label\n        with
          open(Ys[i], \"r\") as f:\n            for source_object in f.readlines():\n                source_object
          = source_object.strip()\n                download_path = f\"/dataset/{splits}/labels/{os.path.basename(source_object)}\"\n                download_from_minio(source_bucket,
          source_object, minio_client, download_path)\n\n    data = {\n        ''path'':
          ''/dataset/'',\n        ''train'': ''train/images'',\n        ''val'': ''val/images'',\n        ''test'':
          ''test/images'',\n        ''names'': {\n            0: ''id_card''\n        }\n    }\n\n    data_yaml_full_path
          = os.path.join(data_yaml_path, \"data.yaml\")\n    from pathlib import Path\n    Path(data_yaml_path).mkdir(parents=True,
          exist_ok=True)\n\n    try:\n        with open(data_yaml_full_path, ''w'')
          as file:\n            yaml.dump(data, file)\n        print(\"YAML file has
          been written successfully.\")\n    except Exception as e:\n        print(f\"Error
          writing YAML file: {e}\")\n\n    from ultralytics import YOLO\n    # from
          ultralytics import settings\n    #\n    # # Update a setting\n    # settings.update({''mlflow'':
          True})\n    #\n    # # Reset settings to default values\n    # settings.reset()\n\n    model
          = YOLO(''yolov8n.pt'')\n\n    results = model.train(\n        data=data_yaml_full_path,\n        imgsz=640,\n        epochs=epochs,\n        batch=batch,\n        project=\"/mnt/pipeline\",\n        name=yolo_model_name,\n   )\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"--epochs\",
          dest=\"epochs\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--batch\",
          dest=\"batch\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--source-bucket\",
          dest=\"source_bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--yolo-model-name\",
          dest=\"yolo_model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--x-train\",
          dest=\"x_train_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-train\",
          dest=\"y_train_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--x-test\",
          dest=\"x_test_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-test\",
          dest=\"y_test_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--x-val\",
          dest=\"x_val_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-val\",
          dest=\"y_val_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-yaml\",
          dest=\"data_yaml_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "ultralytics/ultralytics:8.0.194-cpu"}},
          "inputs": [{"name": "epochs", "type": "Integer"}, {"name": "batch", "type":
          "Integer"}, {"name": "source_bucket", "type": "String"}, {"name": "yolo_model_name",
          "type": "String"}, {"name": "x_train", "type": "String"}, {"name": "y_train",
          "type": "String"}, {"name": "x_test", "type": "String"}, {"name": "y_test",
          "type": "String"}, {"name": "x_val", "type": "String"}, {"name": "y_val",
          "type": "String"}], "name": "Train model", "outputs": [{"name": "data_yaml",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"batch":
          "{{inputs.parameters.batch}}", "epochs": "{{inputs.parameters.epochs}}",
          "source_bucket": "dataset", "yolo_model_name": "{{inputs.parameters.yolo_model_name}}"}'}
    volumes:
    - name: local-storage
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-pvc-name}}'}
  - name: validate-model
    container:
      args: [--data-yaml, /tmp/inputs/data_yaml/data, --yolo-model-name, '{{inputs.parameters.yolo_model_name}}',
        --source-bucket, dataset, --x-val, /tmp/inputs/x_val/data, --y-val, /tmp/inputs/y_val/data,
        --mlpipeline-metrics, /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'minio' 'tqdm' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'minio' 'tqdm' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def validate_model(data_yaml_path,
                           yolo_model_name,
                           source_bucket,
                           x_val_file,
                           y_val_file,
                           mlpipeline_metrics_path):
            import os
            from minio import S3Error, Minio
            from tqdm import tqdm

            def download_from_minio(source_bucket, source_object, minio_client, download_path):
                try:
                    # Download the file from MinIO
                    minio_client.fget_object(source_bucket, source_object, download_path)
                except S3Error as err:
                    print(f"Error downloading {source_object}: {err}")

            minio_client = Minio(
                'minio-service.kubeflow:9000',
                access_key='minio',
                secret_key='minio123',
                secure=False
            )

            # Create local directories.
            for x in ["images", "labels"]:
                os.makedirs(f"/dataset/val/{x}", exist_ok=True)

            X = x_val_file
            Y = y_val_file

            with open(X, "r") as f:
                for source_object in tqdm(f.readlines()):
                    source_object = source_object.strip()
                    download_path = f"/dataset/val/images/{os.path.basename(source_object)}"
                    download_from_minio(source_bucket, source_object, minio_client, download_path)

            # Download label
            with open(Y, "r") as f:
                for source_object in f.readlines():
                    source_object = source_object.strip()
                    download_path = f"/dataset/val/labels/{os.path.basename(source_object)}"
                    download_from_minio(source_bucket, source_object, minio_client, download_path)

            from ultralytics import YOLO
            import json
            import os

            weights_path = os.path.join("mnt", "pipeline", yolo_model_name, "weights", "best.pt")
            print(f"Loading weights at: {weights_path}")

            model = YOLO(weights_path)

            metrics = model.val(
                data=os.path.join(data_yaml_path, "data.yaml")
            )
            metrics_dict = {
                "metrics": [
                    {
                        "name": "map50-95",
                        "numberValue": metrics.box.map,
                        "format": "RAW",
                    },
                    {
                        "name": "map50",
                        "numberValue": metrics.box.map50,
                        "format": "RAW",
                    },
                    {
                        "name": "map75",
                        "numberValue": metrics.box.map75,
                        "format": "RAW",
                    }]
            }

            with open(mlpipeline_metrics_path, "w") as f:
                json.dump(metrics_dict, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate model', description='')
        _parser.add_argument("--data-yaml", dest="data_yaml_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--yolo-model-name", dest="yolo_model_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--source-bucket", dest="source_bucket", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--x-val", dest="x_val_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y-val", dest="y_val_file", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--mlpipeline-metrics", dest="mlpipeline_metrics_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = validate_model(**_parsed_args)
      image: ultralytics/ultralytics:8.0.194-cpu
      volumeMounts:
      - {mountPath: /mnt/pipeline, name: local-storage}
    inputs:
      parameters:
      - {name: create-pvc-name}
      - {name: yolo_model_name}
      artifacts:
      - {name: train-model-data_yaml, path: /tmp/inputs/data_yaml/data}
      - {name: split-dataset-x_val, path: /tmp/inputs/x_val/data}
      - {name: split-dataset-y_val, path: /tmp/inputs/y_val/data}
    outputs:
      artifacts:
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-yaml", {"inputPath": "data_yaml"}, "--yolo-model-name",
          {"inputValue": "yolo_model_name"}, "--source-bucket", {"inputValue": "source_bucket"},
          "--x-val", {"inputPath": "x_val"}, "--y-val", {"inputPath": "y_val"}, "--mlpipeline-metrics",
          {"outputPath": "mlpipeline_metrics"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''minio'' ''tqdm''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''minio'' ''tqdm'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef validate_model(data_yaml_path,\n                   yolo_model_name,\n                   source_bucket,\n                   x_val_file,\n                   y_val_file,\n                   mlpipeline_metrics_path):\n    import
          os\n    from minio import S3Error, Minio\n    from tqdm import tqdm\n\n    def
          download_from_minio(source_bucket, source_object, minio_client, download_path):\n        try:\n            #
          Download the file from MinIO\n            minio_client.fget_object(source_bucket,
          source_object, download_path)\n        except S3Error as err:\n            print(f\"Error
          downloading {source_object}: {err}\")\n\n    minio_client = Minio(\n        ''minio-service.kubeflow:9000'',\n        access_key=''minio'',\n        secret_key=''minio123'',\n        secure=False\n    )\n\n    #
          Create local directories.\n    for x in [\"images\", \"labels\"]:\n        os.makedirs(f\"/dataset/val/{x}\",
          exist_ok=True)\n\n    X = x_val_file\n    Y = y_val_file\n\n    with open(X,
          \"r\") as f:\n        for source_object in tqdm(f.readlines()):\n            source_object
          = source_object.strip()\n            download_path = f\"/dataset/val/images/{os.path.basename(source_object)}\"\n            download_from_minio(source_bucket,
          source_object, minio_client, download_path)\n\n    # Download label\n    with
          open(Y, \"r\") as f:\n        for source_object in f.readlines():\n            source_object
          = source_object.strip()\n            download_path = f\"/dataset/val/labels/{os.path.basename(source_object)}\"\n            download_from_minio(source_bucket,
          source_object, minio_client, download_path)\n\n    from ultralytics import
          YOLO\n    import json\n    import os\n\n    weights_path = os.path.join(\"mnt\",
          \"pipeline\", yolo_model_name, \"weights\", \"best.pt\")\n    print(f\"Loading
          weights at: {weights_path}\")\n\n    model = YOLO(weights_path)\n\n    metrics
          = model.val(\n        data=os.path.join(data_yaml_path, \"data.yaml\")\n    )\n    metrics_dict
          = {\n        \"metrics\": [\n            {\n                \"name\": \"map50-95\",\n                \"numberValue\":
          metrics.box.map,\n                \"format\": \"RAW\",\n            },\n            {\n                \"name\":
          \"map50\",\n                \"numberValue\": metrics.box.map50,\n                \"format\":
          \"RAW\",\n            },\n            {\n                \"name\": \"map75\",\n                \"numberValue\":
          metrics.box.map75,\n                \"format\": \"RAW\",\n            }]\n    }\n\n    with
          open(mlpipeline_metrics_path, \"w\") as f:\n        json.dump(metrics_dict,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          model'', description='''')\n_parser.add_argument(\"--data-yaml\", dest=\"data_yaml_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--yolo-model-name\",
          dest=\"yolo_model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--source-bucket\",
          dest=\"source_bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--x-val\",
          dest=\"x_val_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y-val\",
          dest=\"y_val_file\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--mlpipeline-metrics\",
          dest=\"mlpipeline_metrics_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = validate_model(**_parsed_args)\n"], "image": "ultralytics/ultralytics:8.0.194-cpu"}},
          "inputs": [{"name": "data_yaml", "type": "String"}, {"name": "yolo_model_name",
          "type": "String"}, {"name": "source_bucket", "type": "String"}, {"name":
          "x_val", "type": "String"}, {"name": "y_val", "type": "String"}], "name":
          "Validate model", "outputs": [{"name": "mlpipeline_metrics", "type": "Metrics"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"source_bucket":
          "dataset", "yolo_model_name": "{{inputs.parameters.yolo_model_name}}"}'}
    volumes:
    - name: local-storage
      persistentVolumeClaim: {claimName: '{{inputs.parameters.create-pvc-name}}'}
  - name: yolo-object-detection-pipeline
    inputs:
      parameters:
      - {name: batch}
      - {name: epochs}
      - {name: random_state}
      - {name: source_bucket}
      - {name: yolo_model_name}
    dag:
      tasks:
      - {name: create-pvc, template: create-pvc}
      - name: split-dataset
        template: split-dataset
        arguments:
          parameters:
          - {name: random_state, value: '{{inputs.parameters.random_state}}'}
          - {name: source_bucket, value: '{{inputs.parameters.source_bucket}}'}
      - name: train-model
        template: train-model
        dependencies: [create-pvc, split-dataset]
        arguments:
          parameters:
          - {name: batch, value: '{{inputs.parameters.batch}}'}
          - {name: create-pvc-name, value: '{{tasks.create-pvc.outputs.parameters.create-pvc-name}}'}
          - {name: epochs, value: '{{inputs.parameters.epochs}}'}
          - {name: yolo_model_name, value: '{{inputs.parameters.yolo_model_name}}'}
          artifacts:
          - {name: split-dataset-x_test, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-x_test}}'}
          - {name: split-dataset-x_train, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-x_train}}'}
          - {name: split-dataset-x_val, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-x_val}}'}
          - {name: split-dataset-y_test, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-y_test}}'}
          - {name: split-dataset-y_train, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-y_train}}'}
          - {name: split-dataset-y_val, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-y_val}}'}
      - name: validate-model
        template: validate-model
        dependencies: [create-pvc, split-dataset, train-model]
        arguments:
          parameters:
          - {name: create-pvc-name, value: '{{tasks.create-pvc.outputs.parameters.create-pvc-name}}'}
          - {name: yolo_model_name, value: '{{inputs.parameters.yolo_model_name}}'}
          artifacts:
          - {name: split-dataset-x_val, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-x_val}}'}
          - {name: split-dataset-y_val, from: '{{tasks.split-dataset.outputs.artifacts.split-dataset-y_val}}'}
          - {name: train-model-data_yaml, from: '{{tasks.train-model.outputs.artifacts.train-model-data_yaml}}'}
  arguments:
    parameters:
    - {name: epochs, value: '1'}
    - {name: batch, value: '8'}
    - {name: source_bucket, value: dataset}
    - {name: random_state, value: '42'}
    - {name: yolo_model_name, value: yolov8n_custom}
  serviceAccountName: pipeline-runner
